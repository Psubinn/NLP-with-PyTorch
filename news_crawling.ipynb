{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2021/01/01 ~ 2021/01/16 의 뉴스를 긁어와서\\\n",
    "- media : 언론사\\\n",
    "- scr : url\\\n",
    "- title : 제목\\\n",
    "- date : 날짜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup as BS\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "\n",
    "\"\"\" 네이버 랭킹 뉴스 긁어오기 \"\"\"\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/84.0.4147.135 Safari/537.36'}\n",
    "\n",
    "d_list = []\n",
    "start_data = 20230101\n",
    "end_data = 20230113\n",
    "for date_int in range(start_data, end_data):\n",
    "    date = str(date_int)\n",
    "    url = \"https://news.naver.com/main/ranking/popularDay.nhn?date=\" + date\n",
    "    html = requests.get(url, headers=headers).text\n",
    "    soup = BS(html, 'html.parser')\n",
    "    ranking_total = soup.find_all(class_='rankingnews_box')\n",
    "\n",
    "    for item in ranking_total:\n",
    "        media = item.a.strong.text\n",
    "        news = item.find_all(class_=\"list_content\")\n",
    "        for new in news:\n",
    "            d = {}\n",
    "            d['media'] = media\n",
    "            d['src'] = \"https://news.naver.com/\" + new.a['href']\n",
    "            d['title'] = new.a.text\n",
    "            d['date'] = date\n",
    "            d_list.append(d)\n",
    "df = pd.DataFrame(d_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" 필요 없는 문자 제거 \"\"\"\n",
    "def clean_text(row):\n",
    "    text = row['title']\n",
    "    pattern = '([a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+)'\n",
    "    text = re.sub(pattern=pattern, repl='', string=text)\n",
    "    # print(\"E-mail제거 : \" , text , \"\\n\")\n",
    "    pattern = '(http|ftp|https)://(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+'\n",
    "    text = re.sub(pattern=pattern, repl='', string=text)\n",
    "    # print(\"URL 제거 : \", text , \"\\n\")\n",
    "    pattern = '([ㄱ-ㅎㅏ-ㅣ]+)'\n",
    "    text = re.sub(pattern=pattern, repl='', string=text)\n",
    "    # print(\"한글 자음 모음 제거 : \", text , \"\\n\")\n",
    "    pattern = '<[^>]*>'\n",
    "    text = re.sub(pattern=pattern, repl='', string=text)\n",
    "    # print(\"태그 제거 : \" , text , \"\\n\")\n",
    "    pattern = r'\\([^)]*\\)'\n",
    "    text = re.sub(pattern=pattern, repl='', string=text)\n",
    "    # print(\"괄호와 괄호안 글자 제거 :  \" , text , \"\\n\")\n",
    "    pattern = '[^\\w\\s]'\n",
    "    text = re.sub(pattern=pattern, repl='', string=text)\n",
    "    # print(\"특수기호 제거 : \", text , \"\\n\" )\n",
    "    pattern = '[^\\w\\s]'\n",
    "    text = re.sub(pattern=pattern, repl='', string=text)\n",
    "    # print(\"필요없는 정보 제거 : \", text , \"\\n\" )\n",
    "    pattern = '[\"단독\"]'\n",
    "    text = re.sub(pattern=pattern, repl='', string=text)\n",
    "    pattern = '[\"속보\"]'\n",
    "    text = re.sub(pattern=pattern, repl='', string=text)\n",
    "    # print(\"단독 속보 제거 : \", text , \"\\n\" )\n",
    "    text = text.strip()\n",
    "    # print(\"양 끝 공백 제거 : \", text , \"\\n\" )\n",
    "    text = \" \".join(text.split())\n",
    "    # print(\"중간에 공백은 1개만 : \", text )\n",
    "    return text\n",
    "\n",
    "\n",
    "df['title_c'] = df.apply(clean_text, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" 키워드 추출 from title \"\"\"\n",
    "from konlpy.tag import Kkma\n",
    "from konlpy.tag import Komoran\n",
    "\n",
    "\n",
    "kkma = Kkma()\n",
    "komoran = Komoran()\n",
    "\n",
    "df['keyword'] = ''\n",
    "for idx_line in range(len(df)):\n",
    "    nouns_list = komoran.nouns(df['title_c'].loc[idx_line])\n",
    "    nouns_list_c = [nouns for nouns in nouns_list if len(nouns) > 1]   # 한글자는 이상한게 많아서 2글자 이상\n",
    "    a = set(nouns_list_c)\n",
    "    df.loc[[idx_line], 'keyword'] = a\n",
    "df = df[df['media'] != '코리아헤럴드']    # 코리아헤럴드는 영어 기사"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "network graph\\\n",
    "- network type : undirected weighted network\n",
    "- node : 키워드\n",
    "- edge : 동일 기사 제목에 등장한 키워드들은 관련이 있을 확률이 높으므로, 키워드들 간에 link 추거. 다른 기사에서 또 다시 link 추가될 시 weight 1 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<networkx.classes.graph.Graph at 0x21c5a3987c8>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" Edge list 작성 \"\"\"\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "edge_list = []\n",
    "for keywords_dict in df['keyword']:\n",
    "    keywords = list(keywords_dict)\n",
    "    num_keyword = len(keywords)\n",
    "    if num_keyword > 0:\n",
    "        for i in range(num_keyword-1):\n",
    "            for j in range(i+1, num_keyword):\n",
    "                edge_list += [tuple(sorted([keywords[i], keywords[j]]))]    # node 간 위해 sorted 사용\n",
    "edges = list(Counter(edge_list).items())\n",
    "\n",
    "\n",
    "\n",
    "\"\"\" networkx Graph 작성 \"\"\"\n",
    "import networkx as nx\n",
    "\n",
    "\n",
    "G = nx.Graph((x, y, {'weight': v}) for (x, y), v in edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Community 추출 \"\"\"\n",
    "#%pip install python-louvain\n",
    "#%pip install networkx\n",
    "#%pip install cdlib\n",
    "\n",
    "from cdlib import algorithms\n",
    "import networkx as nx\n",
    "coms = algorithms.louvain(G, resolution=1., randomize=False)\n",
    "\n",
    "nx.set_node_attributes(G, coms, \"community\")   # graph G에 community 속성 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Gephi file 작성 \"\"\"\n",
    "nx.write_gexf(G, '202301_keyword_community.gexf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ac52480b588c30d04918775713f8eba83fa94f86a06accbc60434e6510854895"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
